{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3813612361.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 42\u001b[0;36m\u001b[0m\n\u001b[0;31m    dfbias.loc[year] = [year] + [yearly_similarities[col] for col in colnames[1:]]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from extract_cosine_similarities import load_dict , load_word2vec_models, calculate_cosine_similarities\n",
    "from calculate_embedding_bias import create_bias_dataframe\n",
    "from plot_results import plot_results\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Get the current notebook path\n",
    "notebook_path = pathlib.Path.cwd()\n",
    "\n",
    "# Create paths to the data and output folders\n",
    "data_folder_path = notebook_path / '..' / '0_data'\n",
    "output_folder_path = notebook_path / '..' / '2_pipeline' / 'preprocessed'\n",
    "\n",
    "# Resolve and convert the paths to strings\n",
    "data_folder_path = data_folder_path.resolve().as_posix()\n",
    "output_folder_path = output_folder_path.resolve().as_posix()\n",
    "foundations_path = os.path.join(data_folder_path, 'wordlist', 'dict_mft.json')\n",
    "model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# Targets (e.g., age groups)\n",
    "age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "foundations = load_dict(foundations_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "# models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# # Change or add foundations to the dictionary here as needed\n",
    "# # For example:\n",
    "# # foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "\n",
    "# # Calculate cosine similarities\n",
    "# similarities = calculate_cosine_similarities(models, targets, foundations)\n",
    "\n",
    "# # Calculate embedding bias\n",
    "# colnames = ['year'] + [f'{target}_{foundation}' for foundation in foundations for target in targets]\n",
    "# dfbias = pd.DataFrame(columns=colnames, index=range(1950, 2022))\n",
    "# for year, yearly_similarities in similarities.items():\n",
    "    # dfbias.loc[year] = [year] + [yearly_similarities[col] for col in colnames[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames = ['year'] + [f'{target}_{foundation}' for foundation in foundations for target in targets]\n",
    "dfbias = pd.DataFrame(columns=colnames, index=range(1950, 2022))\n",
    "for year, yearly_similarities in similarities.items():\n",
    "   dfbias.loc[year] = [year] + [yearly_similarities[target][foundation] for col in colnames[1:] for target, foundation in [col.split('_', 1)]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Calculate embedding bias\u001b[39;00m\n\u001b[1;32m     33\u001b[0m dfbias \u001b[39m=\u001b[39m create_bias_dataframe(similarities, targets, evaluations)\n\u001b[0;32m---> 34\u001b[0m df_embedding_bias \u001b[39m=\u001b[39m calculate_embedding_bias(dfbias, targets, evaluations)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Save the DataFrame as a CSV\u001b[39;00m\n\u001b[1;32m     37\u001b[0m csv_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pipeline_folder_path, name\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Github/agebias-chi/1_code/calculate_embedding_bias.py:12\u001b[0m, in \u001b[0;36mcalculate_embedding_bias\u001b[0;34m(dfbias, targets, foundations)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_embedding_bias\u001b[39m(dfbias: pd\u001b[39m.\u001b[39mDataFrame, targets: \u001b[39mdict\u001b[39m, foundations: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m---> 12\u001b[0m    \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m key \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m foundations\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m     13\u001b[0m         \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m targets:\n\u001b[1;32m     14\u001b[0m                 \u001b[39mfor\u001b[39;00m foundation \u001b[39min\u001b[39;00m foundations:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from extract_cosine_similarities import load_dict , load_word2vec_models, calculate_cosine_similarities\n",
    "from calculate_embedding_bias import create_bias_dataframe\n",
    "\n",
    "\n",
    "# data_folder_path = os.path.join(script_dir, '..', '0_data')\n",
    "# pipeline_folder_path = os.path.join(script_dir, '..', '2_pipeline/preprocessed')\n",
    "# tmp_folder_path = os.path.join(script_dir, '..', '2_pipeline/tmp')\n",
    "\n",
    "# foundations_path = os.path.join(data_folder_path, 'wordlist', 'dict_mft.json')\n",
    "posneg_path = os.path.join(data_folder_path, 'wordlist', 'dict_posneg.json')\n",
    "scm_path = os.path.join(data_folder_path, 'wordlist', 'dict_scm.json')\n",
    "# model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# # Targets (e.g., age groups)\n",
    "# age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "# foundations = load_dict(foundations_path)\n",
    "posneg = load_dict(posneg_path)\n",
    "scm = load_dict(scm_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# Change or add foundations to the dictionary here as needed\n",
    "# For example:\n",
    "# foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "evaluations = {'foundations': foundations,  'scm': scm,'posneg': posneg,}\n",
    "for name, evaluations in evaluations.items():\n",
    "    # Calculate cosine similarities\n",
    "    similarities = calculate_cosine_similarities(models, targets, evaluations)\n",
    "\n",
    "    # Calculate embedding bias\n",
    "    dfbias = create_bias_dataframe(similarities, targets, evaluations)\n",
    "    df_embedding_bias = calculate_embedding_bias(dfbias, targets, evaluations)\n",
    "\n",
    "    # Save the DataFrame as a CSV\n",
    "    csv_filepath = os.path.join(pipeline_folder_path, name+ '.csv')\n",
    "    df_embedding_bias.to_csv(csv_filepath, index=False)\n",
    "model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# Targets (e.g., age groups)\n",
    "age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "foundations = load_dict(foundations_path)\n",
    "posneg = load_dict(posneg_path)\n",
    "scm = load_dict(scm_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# Change or add foundations to the dictionary here as needed\n",
    "# For example:\n",
    "# foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "evaluations = {'scm': scm,'posneg': posneg,}\n",
    "for name, evaluations in evaluations.items():\n",
    "    # Calculate cosine similarities\n",
    "    similarities = calculate_cosine_similarities(models, targets, evaluations)\n",
    "\n",
    "    # Calculate embedding bias\n",
    "    dfbias = create_bias_dataframe(similarities, targets, evaluations)\n",
    "    \n",
    "    # Save the DataFrame as a CSV\n",
    "    csv_filepath = os.path.join(pipeline_folder_path, name+ '.csv')\n",
    "    df_bias.to_csv(csv_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(\"_\" in key for key in scm.keys()):\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'old_care_vir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [yearly_similarities[col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m colnames[\u001b[39m1\u001b[39m:]]\n",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [yearly_similarities[col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m colnames[\u001b[39m1\u001b[39m:]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'old_care_vir'"
     ]
    }
   ],
   "source": [
    "[yearly_similarities[col] for col in colnames[1:]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
