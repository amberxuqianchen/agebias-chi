{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from extract_cosine_similarities import load_dict , load_word2vec_models, calculate_cosine_similarities\n",
    "from calculate_embedding_bias import create_bias_dataframe\n",
    "from plot_results import plot_results\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Get the current notebook path\n",
    "notebook_path = pathlib.Path.cwd()\n",
    "\n",
    "# Create paths to the data and output folders\n",
    "data_folder_path = notebook_path / '..' / '0_data'\n",
    "output_folder_path = notebook_path / '..' / '2_pipeline' / 'preprocessed'\n",
    "pipeline_out_folder_path = notebook_path / '..' / '2_pipeline' / 'out'\n",
    "# Resolve and convert the paths to strings\n",
    "data_folder_path = data_folder_path.resolve().as_posix()\n",
    "output_folder_path = output_folder_path.resolve().as_posix()\n",
    "foundations_path = os.path.join(data_folder_path, 'wordlist', 'dict_mft.json')\n",
    "model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# Targets (e.g., age groups)\n",
    "age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "foundations = load_dict(foundations_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "# models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# # Change or add foundations to the dictionary here as needed\n",
    "# # For example:\n",
    "# # foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "\n",
    "# # Calculate cosine similarities\n",
    "# similarities = calculate_cosine_similarities(models, targets, foundations)\n",
    "\n",
    "# # Calculate embedding bias\n",
    "# colnames = ['year'] + [f'{target}_{foundation}' for foundation in foundations for target in targets]\n",
    "# dfbias = pd.DataFrame(columns=colnames, index=range(1950, 2022))\n",
    "# for year, yearly_similarities in similarities.items():\n",
    "    # dfbias.loc[year] = [year] + [yearly_similarities[col] for col in colnames[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames = ['year'] + [f'{target}_{foundation}' for foundation in foundations for target in targets]\n",
    "dfbias = pd.DataFrame(columns=colnames, index=range(1950, 2022))\n",
    "for year, yearly_similarities in similarities.items():\n",
    "   dfbias.loc[year] = [year] + [yearly_similarities[target][foundation] for col in colnames[1:] for target, foundation in [col.split('_', 1)]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# Calculate embedding bias\u001b[39;00m\n\u001b[1;32m     33\u001b[0m dfbias \u001b[39m=\u001b[39m create_bias_dataframe(similarities, targets, evaluations)\n\u001b[0;32m---> 34\u001b[0m df_embedding_bias \u001b[39m=\u001b[39m calculate_embedding_bias(dfbias, targets, evaluations)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Save the DataFrame as a CSV\u001b[39;00m\n\u001b[1;32m     37\u001b[0m csv_filepath \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(pipeline_folder_path, name\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Github/agebias-chi/1_code/calculate_embedding_bias.py:12\u001b[0m, in \u001b[0;36mcalculate_embedding_bias\u001b[0;34m(dfbias, targets, foundations)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_embedding_bias\u001b[39m(dfbias: pd\u001b[39m.\u001b[39mDataFrame, targets: \u001b[39mdict\u001b[39m, foundations: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m pd\u001b[39m.\u001b[39mDataFrame:\n\u001b[0;32m---> 12\u001b[0m    \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m key \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m foundations\u001b[39m.\u001b[39mkeys()):\n\u001b[1;32m     13\u001b[0m         \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m targets:\n\u001b[1;32m     14\u001b[0m                 \u001b[39mfor\u001b[39;00m foundation \u001b[39min\u001b[39;00m foundations:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from extract_cosine_similarities import load_dict , load_word2vec_models, calculate_cosine_similarities\n",
    "from calculate_embedding_bias import create_bias_dataframe\n",
    "\n",
    "\n",
    "# data_folder_path = os.path.join(script_dir, '..', '0_data')\n",
    "# pipeline_folder_path = os.path.join(script_dir, '..', '2_pipeline/preprocessed')\n",
    "# tmp_folder_path = os.path.join(script_dir, '..', '2_pipeline/tmp')\n",
    "\n",
    "# foundations_path = os.path.join(data_folder_path, 'wordlist', 'dict_mft.json')\n",
    "posneg_path = os.path.join(data_folder_path, 'wordlist', 'dict_posneg.json')\n",
    "scm_path = os.path.join(data_folder_path, 'wordlist', 'dict_scm.json')\n",
    "# model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# # Targets (e.g., age groups)\n",
    "# age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "# foundations = load_dict(foundations_path)\n",
    "posneg = load_dict(posneg_path)\n",
    "scm = load_dict(scm_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# Change or add foundations to the dictionary here as needed\n",
    "# For example:\n",
    "# foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "evaluations = {'foundations': foundations,  'scm': scm,'posneg': posneg,}\n",
    "for name, evaluations in evaluations.items():\n",
    "    # Calculate cosine similarities\n",
    "    similarities = calculate_cosine_similarities(models, targets, evaluations)\n",
    "\n",
    "    # Calculate embedding bias\n",
    "    dfbias = create_bias_dataframe(similarities, targets, evaluations)\n",
    "    df_embedding_bias = calculate_embedding_bias(dfbias, targets, evaluations)\n",
    "\n",
    "    # Save the DataFrame as a CSV\n",
    "    csv_filepath = os.path.join(pipeline_folder_path, name+ '.csv')\n",
    "    df_embedding_bias.to_csv(csv_filepath, index=False)\n",
    "model_folder_path = os.path.join(data_folder_path, 'model')\n",
    "\n",
    "# Targets (e.g., age groups)\n",
    "age_groups_path = os.path.join(data_folder_path, 'wordlist', 'age_groups.json')\n",
    "\n",
    "foundations = load_dict(foundations_path)\n",
    "posneg = load_dict(posneg_path)\n",
    "scm = load_dict(scm_path)\n",
    "targets = load_dict(age_groups_path)\n",
    "\n",
    "models = load_word2vec_models(model_folder_path)\n",
    "\n",
    "# Change or add foundations to the dictionary here as needed\n",
    "# For example:\n",
    "# foundations['new_foundation_name'] = ['word1', 'word2', 'word3']\n",
    "evaluations = {'scm': scm,'posneg': posneg,}\n",
    "for name, evaluations in evaluations.items():\n",
    "    # Calculate cosine similarities\n",
    "    similarities = calculate_cosine_similarities(models, targets, evaluations)\n",
    "\n",
    "    # Calculate embedding bias\n",
    "    dfbias = create_bias_dataframe(similarities, targets, evaluations)\n",
    "    \n",
    "    # Save the DataFrame as a CSV\n",
    "    csv_filepath = os.path.join(pipeline_folder_path, name+ '.csv')\n",
    "    df_bias.to_csv(csv_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(\"_\" in key for key in scm.keys()):\n",
    "    print(\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'old_care_vir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [yearly_similarities[col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m colnames[\u001b[39m1\u001b[39m:]]\n",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m [yearly_similarities[col] \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m colnames[\u001b[39m1\u001b[39m:]]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'old_care_vir'"
     ]
    }
   ],
   "source": [
    "[yearly_similarities[col] for col in colnames[1:]]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# granger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.var_model import VARResults\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "from statsmodels.tsa.vector_ar.var_model import VARProcess\n",
    "\n",
    "def granger_causality_matrix(data, variables, test='ssr_chi2test', verbose=False):\n",
    "    X_train = pd.DataFrame(data)\n",
    "    n = X_train.shape[1]\n",
    "    granger_matrix = np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                continue\n",
    "            data = X_train[[variables[i], variables[j]]]\n",
    "            result = grangercausalitytests(data, maxlag=12, verbose=False)\n",
    "            p_values = [round(result[i+1][0][test][1],4) for i in range(12)]\n",
    "            if verbose:\n",
    "                print(f'Y = {variables[i]}, X = {variables[j]}, P Values = {p_values}')\n",
    "            min_p_value = np.min(p_values)\n",
    "            granger_matrix[i,j] = min_p_value\n",
    "    return granger_matrix\n",
    "\n",
    "def run_granger_test(df):\n",
    "    model = VAR(df)\n",
    "    results = model.fit(maxlags=15, ic='aic')\n",
    "    lag_order = results.k_ar\n",
    "    vecm_model = results.vecm(0)\n",
    "    vecm_model.summary()\n",
    "    vecm_model.test_granger_causality(caused='GDP', causing='indicoll', kind='f', signif=0.05).summary()\n",
    "\n",
    "\n",
    "df=pd.read_csv(os.path.join(pipeline_out_folder_path, 'merged.csv'))\n",
    "df.set_index('year', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 23.5.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/local/PSYCH-ADS/xuqian_chen/anaconda3/envs/jupyter_env/envs/ngram\n",
      "\n",
      "  added / updated specs:\n",
      "    - pmdarima\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2023.5.7           |     pyhd8ed1ab_0         149 KB  conda-forge\n",
      "    cython-0.29.33             |   py39h6a678d5_0         2.1 MB\n",
      "    joblib-1.2.0               |     pyhd8ed1ab_0         205 KB  conda-forge\n",
      "    libblas-3.9.0              |   12_linux64_mkl          12 KB  conda-forge\n",
      "    libcblas-3.9.0             |   12_linux64_mkl          12 KB  conda-forge\n",
      "    liblapack-3.9.0            |   12_linux64_mkl          12 KB  conda-forge\n",
      "    numpy-1.22.3               |   py39hc58783e_2         6.8 MB  conda-forge\n",
      "    pmdarima-2.0.3             |   py39h5eee18b_0         604 KB\n",
      "    scikit-learn-1.0.2         |   py39h4dfa638_0        26.8 MB  conda-forge\n",
      "    threadpoolctl-3.1.0        |     pyh8a188c0_0          18 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        36.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cython             pkgs/main/linux-64::cython-0.29.33-py39h6a678d5_0\n",
      "  joblib             conda-forge/noarch::joblib-1.2.0-pyhd8ed1ab_0\n",
      "  libblas            conda-forge/linux-64::libblas-3.9.0-12_linux64_mkl\n",
      "  libcblas           conda-forge/linux-64::libcblas-3.9.0-12_linux64_mkl\n",
      "  liblapack          conda-forge/linux-64::liblapack-3.9.0-12_linux64_mkl\n",
      "  pmdarima           pkgs/main/linux-64::pmdarima-2.0.3-py39h5eee18b_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39\n",
      "  scikit-learn       conda-forge/linux-64::scikit-learn-1.0.2-py39h4dfa638_0\n",
      "  threadpoolctl      conda-forge/noarch::threadpoolctl-3.1.0-pyh8a188c0_0\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  numpy-base-1.21.2-py39h79a1101_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2023.5.7-hbcca054_0\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2023.5.7-pyhd8ed1ab_0\n",
      "  numpy              pkgs/main::numpy-1.21.2-py39h20f2e39_0 --> conda-forge::numpy-1.22.3-py39hc58783e_2\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "scikit-learn-1.0.2   | 26.8 MB   | ##################################### | 100% \n",
      "joblib-1.2.0         | 205 KB    | ##################################### | 100% \n",
      "cython-0.29.33       | 2.1 MB    | ##################################### | 100% \n",
      "threadpoolctl-3.1.0  | 18 KB     | ##################################### | 100% \n",
      "numpy-1.22.3         | 6.8 MB    | ##################################### | 100% \n",
      "liblapack-3.9.0      | 12 KB     | ##################################### | 100% \n",
      "libcblas-3.9.0       | 12 KB     | ##################################### | 100% \n",
      "pmdarima-2.0.3       | 604 KB    | ##################################### | 100% \n",
      "certifi-2023.5.7     | 149 KB    | ##################################### | 100% \n",
      "libblas-3.9.0        | 12 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install pmdarima -c conda-forge -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x contains NaN or inf values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m residuals\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mresid()\n\u001b[1;32m      6\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df[\u001b[39m'\u001b[39m\u001b[39mindi\u001b[39m\u001b[39m'\u001b[39m],residuals])\n\u001b[0;32m----> 7\u001b[0m granger_test\u001b[39m=\u001b[39mgrangercausalitytests(data, maxlag\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(granger_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/jupyter_env/envs/ngram/lib/python3.9/site-packages/statsmodels/tsa/stattools.py:1459\u001b[0m, in \u001b[0;36mgrangercausalitytests\u001b[0;34m(x, maxlag, addconst, verbose)\u001b[0m\n\u001b[1;32m   1457\u001b[0m x \u001b[39m=\u001b[39m array_like(x, \u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m, ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misfinite(x)\u001b[39m.\u001b[39mall():\n\u001b[0;32m-> 1459\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mx contains NaN or inf values.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1460\u001b[0m addconst \u001b[39m=\u001b[39m bool_like(addconst, \u001b[39m\"\u001b[39m\u001b[39maddconst\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1461\u001b[0m verbose \u001b[39m=\u001b[39m bool_like(verbose, \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x contains NaN or inf values."
     ]
    }
   ],
   "source": [
    "import pmdarima as pm\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "model=pm.auto_arima(df['old_positive'],seasonal=False)\n",
    "residuals=model.resid()\n",
    "# concatenate residuals and independent variable and delete missing values\n",
    "\n",
    "data = pd.concat([df['indi'],residuals])\n",
    "granger_test=grangercausalitytests(data, maxlag=10, verbose=False)\n",
    "\n",
    "print(granger_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "1950    0.000211\n",
       "1951    0.000195\n",
       "1952    0.000191\n",
       "1953    0.000205\n",
       "1954    0.000205\n",
       "          ...   \n",
       "2017    0.001363\n",
       "2018    0.001330\n",
       "2019         NaN\n",
       "2020         NaN\n",
       "2021         NaN\n",
       "Name: indi, Length: 72, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['indi']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
